{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO819F4sjbw+XI/Ak73M47r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 07MAIR - Redes Neuronales y Deep Learning\n","## VC10: Más allá: Visualización e Interpretabilidad"],"metadata":{"id":"5CcRr9jevWq3"}},{"cell_type":"code","source":["# SOLO PARA USO EN GOOGLE COLABORATORY\n","# Para conectar el notebook con la cuenta de gdrive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","BASE_FOLDER = '/content/drive/My Drive/VIU/07_RN_MIAR/03.Materiales_del_profesor/' # Se debe garantizar que la carpeta docencia compartida se almacena en el directorio raíz de Google Drive. En caso contrario modificar este path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uiQrK-U0vVUQ","executionInfo":{"status":"ok","timestamp":1698252742459,"user_tz":240,"elapsed":946,"user":{"displayName":"Julio Silva","userId":"03943073890214428625"}},"outputId":"9b3762ec-75e4-4b3a-8813-9863b87154d8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## **VISUALIZANDO POR DENTRO UNA CNN**"],"metadata":{"id":"riljOWN2vDP_"}},{"cell_type":"markdown","source":["#### **- Visualizar activaciones intermedias**"],"metadata":{"id":"bWazpkQqvAaV"}},{"cell_type":"markdown","source":["- Visualizar el output de las capas de la red\n","- 2D imagen por canal"],"metadata":{"id":"3_v_9YOsvJur"}},{"cell_type":"code","source":["from tensorflow.keras.datasets import cifar10\n","from sklearn.preprocessing import LabelBinarizer\n","\n","# Importando y normalizando el set de datos CIFAR10\n","print(\"[INFO]: Loading CIFAR-10 data...\")\n","((trainX, trainY), (testX, testY)) = cifar10.load_data()\n","labelNames = [\"Avión\", \"Automóvil\", \"Pájaro\", \"Gato\", \"Ciervo\", \"Perro\", \"Rana\", \"Caballo\", \"Barco\", \"Camión\"]\n","\n","#One-hot encoding\n","lb = LabelBinarizer()\n","trainY = lb.fit_transform(trainY)\n","testY = lb.transform(testY)"],"metadata":{"id":"e_hpUfi1vLTO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import load_model\n","model_augmentation = load_model(BASE_FOLDER+'resources/convnet_augmentation.h5')\n","model_augmentation.summary()"],"metadata":{"id":"SVcbz98BvqEe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extraer outputs\n","from keras import models\n","\n","output_layers = [layer.output for layer in model_augmentation.layers[:7]]\n","activation_model = models.Model(inputs=model_augmentation.input, outputs=output_layers)"],"metadata":{"id":"vViA2q2WwIUj","executionInfo":{"status":"ok","timestamp":1698252897519,"user_tz":240,"elapsed":106,"user":{"displayName":"Julio Silva","userId":"03943073890214428625"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from tensorflow.keras.preprocessing.image import array_to_img\n","%matplotlib inline\n","\n","# Visualizando imagen\n","sample = 1000\n","input_img = trainX[sample].reshape((1,32,32,3))\n","plt.imshow(array_to_img(trainX[sample]))\n","plt.show()"],"metadata":{"id":"THRKQd5pwKqa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predecir activaciones\n","activations = activation_model.predict(input_img)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GkwaWRp1wMmo","executionInfo":{"status":"ok","timestamp":1698252845745,"user_tz":240,"elapsed":308,"user":{"displayName":"Julio Silva","userId":"03943073890214428625"}},"outputId":"67e2187f-5831-4ab8-89d3-a877cf8daf44"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 54ms/step\n"]}]},{"cell_type":"code","source":["# Activacion, 32 canales, feature map de 30x30\n","layer = 0\n","layer_activation = activations[layer]\n","print(layer_activation.shape)\n","# Mostrar cualquier canal\n","channel = 31\n","plt.matshow(layer_activation[0,:,:,channel], cmap='jet')"],"metadata":{"id":"0mODkMRmwPYd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","# Visualizar todas las capas\n","layer_names = [layer.name for layer in model_augmentation.layers[:7]]\n","images_per_row = 16\n","display_grid = []\n","for layer_name, layer_activation in zip(layer_names,activations):\n","    # Número de features\n","    n_features = layer_activation.shape[-1]\n","    # Tamaño de cada feature\n","    size = layer_activation.shape[1]\n","    # Número de columnas a mostrar\n","    n_cols = n_features // images_per_row\n","\n","    display_grid = np.zeros((size * n_cols, images_per_row * size))\n","\n","    for col in range(n_cols):\n","        for row in range(images_per_row):\n","            channel_img = layer_activation[0,:,:,col * images_per_row + row]\n","            display_grid[col * size : (col + 1) * size,\n","                        row * size : (row + 1) * size] = channel_img\n","    scale = 1. / size\n","    plt.figure(figsize=(scale * display_grid.shape[1],\n","                       scale * display_grid.shape[0]))\n","    plt.title('Activacion de las capas')\n","    plt.grid(False)\n","    plt.imshow(display_grid,aspect='auto',cmap='jet')"],"metadata":{"id":"tCm6whkaw3gS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Interpretacion\n","- Capas iniciales son como detectores de bordes\n","- Capas más profundas son más difíciles de interpretar (abstractas) y tienen información relativa a la clase de imagen\n","- La activación de capas profundas es más dispersa (sparse)"],"metadata":{"id":"QLhlvDy-w9wr"}},{"cell_type":"markdown","source":["#### **- Visualizar filtros convolucionales**"],"metadata":{"id":"hbi7uWHcxDhP"}},{"cell_type":"markdown","source":["- Visualizar las imagenes que maximizan la respuesta a un filtro"],"metadata":{"id":"2dA66fRwxGXU"}},{"cell_type":"code","source":["from keras.applications import VGG16\n","from tensorflow.keras import backend as K\n","\n","model = VGG16(weights='imagenet',include_top=False)\n","model.summary()"],"metadata":{"id":"moT1yaERxJA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def gen_max_response_pattern(layer_name, filter_index, size=32):\n","    layer_output = model.get_layer(layer_name).output\n","    loss = K.mean(layer_output[:,:,:,filter_index])\n","\n","    # Para obtener el patron que responde de forma máxima, utilizamos descenso del gradiente\n","    grads = K.gradients(loss,model.input)[0] # Seleccionar el primero, ya que esto devuelve una lista\n","    # Normalización para ayudar al proceso del gradiente\n","    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5) # Última constante para evitar dividir por 0\n","    # Definir funcion que calcula la pérdida y el gradiente de la imagen\n","    iterate = K.function([model.input], [loss,grads])\n","    loss_value, grads_value = iterate([np.zeros((1,size,size,3))])\n","\n","    # Iniciamos con imagen aleatoria\n","    input_img_data = np.random.random((1,size,size,3)) * 20 + 128\n","\n","    step = 1. # Magnitud de cada actualizacion en el gradiente\n","    n_steps = 40 # Número de iteraciones\n","    for i in range(n_steps):\n","        loss_value, grads_value = iterate([input_img_data])\n","        input_img_data += grads_value * step\n","    img = input_img_data[0]\n","    # Procesar la imagen resultante\n","    img -= img.mean()\n","    img /= img.std() + 1e-5\n","    img *= 0.1\n","    img += 0.5\n","    img = np.clip(img,0,1)\n","    img *= 255\n","    img = np.clip(img,0,255).astype('uint8')\n","\n","    return img"],"metadata":{"id":"sQqNE23IxNg7","executionInfo":{"status":"ok","timestamp":1698253158843,"user_tz":240,"elapsed":120,"user":{"displayName":"Julio Silva","userId":"03943073890214428625"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# mostrar un patron para un filtro\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","layer_name = 'block2_conv1'\n","filter_index = 112 # canal a estudiar\n","\n","img = gen_max_response_pattern(layer_name,filter_index,32)\n","plt.imshow(img)"],"metadata":{"id":"qHhe_iZZxROb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **- Visualizar hetmaps de activación por clase**"],"metadata":{"id":"T67XLrkaxwI3"}},{"cell_type":"markdown","source":["- Útil para averiguar qué partes de la imagen contribuyen más a la decisión\n","- Interesante para saber qué pasa cuando se cometen errores"],"metadata":{"id":"vRhhaATox0pU"}},{"cell_type":"code","source":["# Técnica utiliza gradientes Ramprasaath, R. Selvaraju et al. (2017). https://arxiv.org/abs/1610.02391\n","from keras.applications import VGG16\n","\n","model = VGG16(weights='imagenet',include_top=True)"],"metadata":{"id":"IFk0ERfhx4AZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing import image\n","from keras.applications.vgg16 import preprocess_input, decode_predictions\n","import numpy as np\n","\n","# Cargar imagen de gato\n","img = image.load_img(BASE_FOLDER+'resources/cat.jpg', target_size=(224,224))\n","x = image.img_to_array(img) # a array de (224,224,3)\n","x = np.expand_dims(x, axis=0) # a array de (1,224,224,3)\n","x = preprocess_input(x)"],"metadata":{"id":"BEigGCebx8lU","executionInfo":{"status":"ok","timestamp":1698253230177,"user_tz":240,"elapsed":532,"user":{"displayName":"Julio Silva","userId":"03943073890214428625"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# Testeando el output\n","prediction = model.predict(x)\n","print(decode_predictions(prediction))"],"metadata":{"id":"Jfns2Hi8x-8n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tabby_output_index = np.argmax(prediction[0])\n","tabby_output = model.output[:,tabby_output_index]\n","last_conv_layer = model.get_layer('block5_conv3')"],"metadata":{"id":"_fhuFn3UyAa2","executionInfo":{"status":"ok","timestamp":1698253245869,"user_tz":240,"elapsed":109,"user":{"displayName":"Julio Silva","userId":"03943073890214428625"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","from keras import backend as K\n","\n","grads = K.gradients(tabby_output, last_conv_layer.output)[0]\n","pooled_grads = K.mean(grads, axis=(0,1,2))\n","iterate = K.function( [model.input], [pooled_grads, last_conv_layer.output[0]])\n","pooled_grads_value, conv_layer_output_value = iterate([x])\n","\n","# Multiplicar cada canal en el feature map por como de importante el canal es con respecto a la clase \"tabby\"\n","for i in range(512):\n","    conv_layer_output_value[:,:,i] *= pooled_grads_value[i]\n","\n","heatmap = np.mean(conv_layer_output_value, axis=-1)\n","\n","# Visualizar\n","heatmap = np.maximum(heatmap,0)\n","heatmap /= np.max(heatmap)\n","fig,axes = plt.subplots(1,2)\n","axes[0].matshow(heatmap)\n","axes[1].imshow(img)\n","plt.show()"],"metadata":{"id":"hlLKmq2gyCJY"},"execution_count":null,"outputs":[]}]}