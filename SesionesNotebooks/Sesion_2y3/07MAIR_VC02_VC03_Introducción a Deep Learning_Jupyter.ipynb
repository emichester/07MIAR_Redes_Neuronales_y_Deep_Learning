{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emichester/07MIAR_Redes_Neuronales_y_Deep_Learning/blob/main/SesionesNotebooks/Sesion_2y3/07MAIR_VC02_VC03_Introducci%C3%B3n%20a%20Deep%20Learning_Jupyter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5PHHO92H3-T"
      },
      "source": [
        "<img src=\"https://github.com/emichester/07MIAR_Redes_Neuronales_y_Deep_Learning/blob/main/SesionesNotebooks/img/viu_logo.png?raw=1\" width=\"200\">\n",
        "\n",
        "# 07MAIR - Redes Neuronales y Deep Learning\n",
        "## Clase 02: Redes neuronales artificiales\n",
        "<img src=\"https://github.com/emichester/07MIAR_Redes_Neuronales_y_Deep_Learning/blob/main/SesionesNotebooks/img/keras_logo.jpg?raw=1\" width=\"200\">\n",
        "\n",
        "### Profesor: Julio Silva Rodríguez\n",
        "### Autor: Carlos Fernández Musoles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWTyVy9zH3-V"
      },
      "source": [
        "### Sumario\n",
        "- Funciones de activación\n",
        "- Visualización con Tensorflow playground\n",
        "- Ejemplo mnist keras\n",
        "- Optimización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubao-THfH3-V"
      },
      "source": [
        "<img src=\"https://github.com/emichester/07MIAR_Redes_Neuronales_y_Deep_Learning/blob/main/SesionesNotebooks/img/xor_multi_layer.png?raw=1\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr6GD1uvH3-W"
      },
      "source": [
        "<img src=\"https://github.com/emichester/07MIAR_Redes_Neuronales_y_Deep_Learning/blob/main/SesionesNotebooks/img/xor_multi_layer_solucion.png?raw=1\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ka8Ypc3H3-W"
      },
      "source": [
        "### ¿Diferencias en batch size?\n",
        "- Batch size = 1\n",
        "- Tamaño del training set > Batch size > 1\n",
        "- Batch size = Tamaño del training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNVIdiD1H3-W"
      },
      "source": [
        "#### Batch size = 1\n",
        " - Mucho sesgo en la actualización de pesos -> Outliers  \n",
        " - Tantas actualizaciones de pesos como muestras en el trainig set -> Convergencia más lenta (época lenta)\n",
        " - Poca carga en memoria ya que no se almacenan en RAM conjuntos de muestras ni errores por muestra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8pow-yTH3-X"
      },
      "source": [
        "#### Batch size = Tamaño del training set\n",
        " - Una única actualización de pesos retropropagando el error medio de todas las muestras\n",
        " - Necesidad de establecer mayor número de épocas para garantizar convergencia\n",
        " - La época es más rápida ya que solo hay un paso por época\n",
        " - La RAM debe almacenar todas las muestras y errores asociados -> No es permisible en datasets grandes (OOM!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_iqxpVeH3-Y"
      },
      "source": [
        "#### Tamaño del training set > Batch size > 1\n",
        " - Solución de compromiso entre las dos anteriores\n",
        " - Tamaño de batch suele establecerse como potencia de dos\n",
        " - Pasos por época = int(Tamaño del dataset/Batch size)\n",
        " - Valor exacto del hiperparámetro: ¿arte? -> Ciencia empírica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suEsXv1QH3-Y"
      },
      "source": [
        "# Hiperparámetros en el entrenamiento de ANNs\n",
        "\n",
        "- Época: Una pasada de todos las training samples\n",
        "- Batch size: Agrupación de training samples considerados para hacer una actualización de los parámetros de la red\n",
        "- Learning rate: Magnitud del cambio en cada actualización\n",
        "- Función de activación: Introduce las no linealidades en el esquema de red neuronal\n",
        "- Función de pérdida: Error entre lo esperado (etiqueta, ground truth) y lo predicho en la época actual\n",
        "- Topología de la red neuronal: Número de capas ocultas y unidades/neuronas por capa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh_ZrEClH3-Y"
      },
      "source": [
        "# Tradeoffs:\n",
        "#### Batch size\n",
        " - Alto: Lento en entrenar y mucha memoria RAM consumida, problemático en datasets de grandes dimensiones\n",
        " - Bajo: Poca memoria consumida, errático en bajar el error (mayor sesgo)\n",
        "\n",
        "#### Learning rate\n",
        " - Alto: Problemas para alcanzar minimos globales\n",
        " - Bajo: Lentitud y posible estancamiento en minimo local\n",
        "\n",
        "VIsualización de entrenamiento con TensorFlow playground https://playground.tensorflow.org"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QUGDal3H3-Y"
      },
      "source": [
        "## Función de activación\n",
        "\n",
        "Muchas disponibles: https://en.wikipedia.org/wiki/Activation_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdE264LTH3-Z"
      },
      "source": [
        "### Umbral\n",
        "<img src=\"https://github.com/emichester/07MIAR_Redes_Neuronales_y_Deep_Learning/blob/main/SesionesNotebooks/img/stepfunc.png?raw=1\" width=\"300\">\n",
        "\n",
        "Fuente: Wikipedia https://en.wikipedia.org/wiki/Step_function\n",
        "\n",
        "- Input > umbral = 1, else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ9oSZrtH3-Z"
      },
      "source": [
        "- Difícil convergencia durante entrenamiento\n",
        "- Para multiclase, cómo decidir el output correcto (tres 1 y un 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aounHQdpH3-Z"
      },
      "source": [
        "### Linear\n",
        "\n",
        "<img src=\"https://github.com/emichester/07MIAR_Redes_Neuronales_y_Deep_Learning/blob/main/SesionesNotebooks/img/linear.png?raw=1\" width=\"300\">\n",
        "\n",
        "Fuente: Wikipedia https://en.wikipedia.org/wiki/Activation_function\n",
        "\n",
        "- Input - output linear\n",
        "- Output -inf a +inf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmmiuEaGH3-Z"
      },
      "source": [
        "- Combinación de funciones lineales da una funcion lineal\n",
        "- Explosión de valores (dificil entrenamiento)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKRK5kFhH3-Z"
      },
      "source": [
        "### Sigmoide\n",
        "\n",
        "<img src=\"https://github.com/emichester/07MIAR_Redes_Neuronales_y_Deep_Learning/blob/main/SesionesNotebooks/img/sigmoid.png?raw=1\" width=\"300\">\n",
        "\n",
        "- No linear\n",
        "- Output 0 a 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QKYdu37H3-Z"
      },
      "source": [
        "- Problema: entrenamiento puede ser lento cuando los valores residen en los extremos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "padW_1HbH3-Z"
      },
      "source": [
        "### Tangencial\n",
        "\n",
        "<img src=\"https://github.com/emichester/07MIAR_Redes_Neuronales_y_Deep_Learning/blob/main/SesionesNotebooks/img/tanh.png?raw=1\" width=\"300\">\n",
        "\n",
        "Fuente: Wolfram https://reference.wolfram.com/language/ref/Tanh.html\n",
        "\n",
        "- No linear\n",
        "- Output -1 a 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ShoIeevH3-Z"
      },
      "source": [
        "- Gradiente más fuerte que sigmoide (derivadas son más intensas, por lo que la convergencia puede dificultarse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKCKSGhrH3-Z"
      },
      "source": [
        "### ReLU (Rectified Linear Unit)\n",
        "\n",
        "<img src=\"https://github.com/emichester/07MIAR_Redes_Neuronales_y_Deep_Learning/blob/main/SesionesNotebooks/img/relu.png?raw=1\" width=\"300\">\n",
        "\n",
        "Fuente: Wikipedia https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
        "\n",
        "- No linear\n",
        "- Output 0 a +inf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATV9FvTvH3-Z"
      },
      "source": [
        "- Cualquier función puede aproximarse con una combinacion de ReLUs\n",
        "- Aunque puede explotar (lado positivo) es más eficiente (menos calculos) que sigmoid o tanh\n",
        " - Sigmoid / tanh el cálculo de la función de activacion es más costoso\n",
        " - Con ReLU, muchas serán 0 (sin calculo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlCecKdPH3-Z"
      },
      "source": [
        "### La no linearidad de la funcion de activación es lo que permite a las ANNs aprender decision boundaries que no son lineales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mBOYn90H3-Z"
      },
      "source": [
        "### Variaciones de SGD\n",
        "- Rmsprop: dividir gradiente entre la media de las magnitudes recientes\n",
        "- Momentum: la magnitud del cambio al actualizar parametros depende del gradiente actual y del cambio anterior. velocity = past_vel * momentum + learning_rate * gradient\n",
        "- Otros disponibles en Keras https://keras.io/optimizers/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyvLEG4eH3-Z"
      },
      "source": [
        "# Instalacion de Keras\n",
        "- Instalacion backend (CPU, GPU)\n",
        "```python\n",
        "pip install tensorflow\n",
        "```\n",
        "```python\n",
        "pip install tensorflow-gpu\n",
        "```\n",
        "- Mas información acerca de la instalación:\n",
        "\n",
        " https://www.pyimagesearch.com/2019/12/09/how-to-install-tensorflow-2-0-on-ubuntu/\n",
        "\n",
        " https://www.tensorflow.org/install/gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_ZLceL_H3-Z"
      },
      "source": [
        "## Nuestra primera red neuronal: MNIST dataset\n",
        "Keras y el dataset MNIST para el reconocimiento de digitos escritos a mano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q-w5ZThH3-Z"
      },
      "source": [
        "## MLP aplicado a texto: Ejemplo REUTERS\n",
        "Keras y el dataset MNIST para la clasificación de reseñas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0ao-ctpH3-Z"
      },
      "source": [
        "Hot encoding palabras\n",
        "\n",
        "<img src=\"https://github.com/emichester/07MIAR_Redes_Neuronales_y_Deep_Learning/blob/main/SesionesNotebooks/img/hot_encoding.png?raw=1\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yXERD0SH3-Z"
      },
      "source": [
        "### Ejercicio\n",
        "Comparar los resultados con otros modelos:\n",
        "- Modificar la arquitectura de la red (numero de neuronas, numero de hidden layers)\n",
        "- Demostrar que con menos hidden units se pierde información que no se puede recuperar\n",
        "- Experimentar con 'tanh' en vez de 'relu' como función de activación\n",
        "- Distinto split training / validation (mayoría validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbHGghuYH3-Z"
      },
      "source": [
        "# Optimizacion de redes neuronales\n",
        "- Preprocesado para ANN:\n",
        " - Vectorizacion del input (tensores)\n",
        " - Normalizacion de valores para acelerar entrenamiento (todas las features deben tener misma escala), media 0 y std 1\n",
        " - Valores perdidos: Eliminar, interpolar o utilizar 0 (si 0 no tiene significado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX1cWEcCH3-Z"
      },
      "source": [
        "- Feature engineering (menos importante en deep learning):\n",
        " - Feature cross\n",
        " - Creacion de features basadas en el dominio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPH-2fsaH3-a"
      },
      "source": [
        "- Regularización:\n",
        " - Weight regularization\n",
        " - Dropout\n",
        " - Batch normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iheEjE9RH3-a"
      },
      "source": [
        "# Weight regularization\n",
        "\n",
        "- El modelo más sencillo (menos parametros) que explique los datos es el preferido (Navaja de Ockham)\n",
        "- Lidiar con el problema de overfitting es la tarea más complicada en el paradigma del aprendizaje profundo:\n",
        "- Hace que los parametros tengan valores muy pequeños (coste a cada solución dependiente de la magnitud de los parametros)\n",
        "- Coste proporcional al valor absoluto (L1) o proporcional a la raiz cuadrada del valor (L2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYn_1MHmH3-a"
      },
      "source": [
        "# Dropout\n",
        "\n",
        "- Desactivar neuronas aleatoriamente (i.e. poner a 0 en output features de una capa) -> en cada paso Forward\n",
        "- Dropout rate determina la proporcion de neuronas a desactivar\n",
        "- Funciona al eliminar patrones espurios (evita overfitting)\n",
        "\n",
        "<img src=\"https://github.com/emichester/07MIAR_Redes_Neuronales_y_Deep_Learning/blob/main/SesionesNotebooks/img/dropout.png?raw=1\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvNGTetWH3-a"
      },
      "source": [
        "# Batch normalization\n",
        "\n",
        "[markdown_guide](https://colab.research.google.com/notebooks/markdown_guide.ipynb#scrollTo=Lhfnlq1Surtk)\n",
        "[A Gentle Introduction to Batch Normalization for Deep Neural Networks](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/)\n",
        "\n",
        "- Resta la media (al valor de activación de neurona) y divide entre la desviación típica de los datos del batch -> Estandarización de features\n",
        "    <img src=\"https://github.com/emichester/07MIAR_Redes_Neuronales_y_Deep_Learning/blob/main/SesionesNotebooks/img/batch_norm.png?raw=1\" width=\"600\">\n",
        "\n",
        "    - Tienen una media y varianza móvil normalmente. Y estandarizamos para que las features sigan una distribución normal de μ = 0 y σ = 1\n",
        "    - γ: reescala la activación, es como una función de relevancia\n",
        "    - β: shiftea la media, por ejemplo en vez de tener media cero, una feature que sea más relevante, le pone media 0.4 y le da algo más de prioridad (pero es más sutil)\n",
        "- Se suele situar en la salida de la capa (tras la función de activación)\n",
        "- Mejora el entrenamiento de la red -> Mayor eficiencia computacional\n",
        "- Permite el uso de valores de learning rate más altos, porque al tener valores más escalados los gradientes serán menores (estarán más controlados)\n",
        "- Se puede considerar como un método de regularización aunque no es su principal ventaja. Dropout y L1/L2 aportan mayor regularización\n",
        "- Existen variantes como Layer normalization, Instance normalization o Group normalization (CNNs)\n",
        "<img src=\"https://github.com/emichester/07MIAR_Redes_Neuronales_y_Deep_Learning/blob/main/SesionesNotebooks/img/batch_norm_cnn.png?raw=1\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAQ17HN1H3-a"
      },
      "source": [
        "## Regularización en un MLP: L1/L2, Dropout y Batch Normalization\n",
        "Keras y el dataset Reuters para demostrar el uso de regularización en deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17iRu2hPH3-a"
      },
      "source": [
        "## Función de pérdida / Función de activación (Tabla resumen)\n",
        "\n",
        "- Dependiente del problema\n",
        " - Clasificación binaria: activación ultima capa (sigmoide), función de perdida (binary_crossentropy)\n",
        " - Clasificacion multiclase: activación ultima capa (softmax), función de perdida (categorical_crossentropy)\n",
        " - Regresión con valores arbitrarios: activación ultima capa (none), función de perdida (mse)\n",
        " - Regresión valores 0 a 1: activación ultima capa (sigmoide), función de perdida (mse o binary_crossentropy)\n",
        "\n",
        " Información en Keras https://keras.io/losses/"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}